# Configuración OPTIMIZADA para 100% AISegment Dataset - 20GB VRAM (CONSERVADOR)
# Dataset: 34,425 imágenes completas
# Hardware: GPU con 20GB VRAM (RTX 3090, RTX 4090, A5000, V100 32GB, A100)
# Uso: python main.py train --config aisegment_full_optimized_20gb
#
# ESTRATEGIA: CONSERVADOR (√batch scaling)
# - Batch size 24 (duplicado de 12 para aprovechar VRAM extra)
# - Learning rate 0.00021 (escalado con √2 para estabilidad)
# - Sin warmup (√batch scaling es naturalmente estable)
# - Convergencia suave y predecible
#
# VENTAJAS vs 10GB config:
# - 40-50% más rápido (batch 24 vs 12)
# - Gradientes más estables (mejor Batch Norm)
# - +0.5-1% IoU esperado
# - Tiempo: ~15-18 horas (vs 25-30h)
#
# Objetivo: IoU 88-90% con convergencia estable

experiment:
  name: "AISegment 100% - 20GB VRAM Optimized (Conservador)"
  description: "Entrenamiento con dataset completo - Batch 24, LR escalado √batch"
  mode: "production"

model:
  architecture: "resnet50"
  image_size: 384
  use_pretrained: true
  use_attention: true

dataset:
  type: "aisegment"
  root: "datasets/AISegment"

  # Descarga automática con kagglehub
  auto_download: true
  kaggle_dataset_id: "laurentmih/aisegmentcom-matting-human-datasets"

  # Split train/val (80/20)
  train_val_split: 0.8

  # Dataset completo (sin sampling)
  sampling:
    enabled: false
    mode: "full"

training:
  num_epochs: 100      # Mismo que config 10GB (batch mayor compensa)
  batch_size: 24       # 🔥 DUPLICADO de 12 (aprovecha 20GB VRAM)
  learning_rate: 0.00021  # ⬆️ Escalado √batch: 0.00015 × √2 ≈ 0.00021
  weight_decay: 0.0000005  # Mantener (regularización adecuada)
  num_workers: 10      # ⬆️ Aumentado de 8 (mejor I/O con batch mayor)
  pin_memory: true
  drop_last: true
  gradient_clip_max_norm: 0.5
  mixed_precision: true  # ✅ Mantener (ahorra VRAM adicional)

  scheduler:
    type: "cosine_annealing"
    T_0: 12            # ⬇️ Reducido de 15 (batch mayor = convergencia más rápida)
    T_mult: 2
    eta_min: 0.000005  # LR mínimo para fine-tuning

  # Loss weights optimizados (sin cambios - ya son óptimos)
  loss_weights:
    alpha: 0.7         # BCE
    beta: 1.3          # Dice (overlap crítico)
    gamma: 0.7         # Perceptual (calidad visual)
    delta: 0.5         # Edge (bordes precisos)

validation:
  frequency: 1
  early_stopping:
    enabled: true
    patience: 20       # Mantener (conservador)
    min_delta: 0.00005
  target_metrics:
    iou: 0.88          # Target SOTA
    dice: 0.92
    pixel_accuracy: 0.96

checkpoints:
  save_best: true
  save_last: true
  save_every_n_epochs: 10
  checkpoint_dir: "checkpoints/aisegment_full_optimized_20gb"

logging:
  log_every_n_batches: 20  # ⬇️ Reducido de 25 (monitoreo más frecuente)
  save_plots: true

# Augmentaciones agresivas (sin cambios - ya optimizadas)
augmentation:
  train:
    horizontal_flip: 0.5
    random_rotate90: 0.4

    shift_scale_rotate:
      enabled: true
      shift_limit: 0.12
      scale_limit: 0.25
      rotate_limit: 20
      p: 0.5

    brightness_contrast:
      enabled: true
      brightness_limit: 0.25
      contrast_limit: 0.25
      p: 0.5

    hue_saturation_value:
      enabled: true
      hue_shift_limit: 15
      sat_shift_limit: 20
      val_shift_limit: 15
      p: 0.3

    gaussian_blur:
      enabled: true
      blur_limit: [3, 7]
      p: 0.2

    random_gamma:
      enabled: true
      gamma_limit: [80, 120]
      p: 0.3

    gaussian_noise:
      enabled: true
      var_limit: [10.0, 30.0]
      p: 0.2

# 📊 MEJORAS ESPERADAS vs 10GB config:
# - IoU esperado: 88-90% (mismo objetivo, convergencia más estable)
# - Velocidad: 40-50% más rápido
#   * Batch 24 vs 12 = ~2x throughput teórico
#   * Overhead reduce ganancia a ~1.5x real
# - Calidad gradientes: +15-20% estabilidad (Batch Norm con 24 samples)
# - Tiempo total: 15-18 horas (vs 25-30h con batch 12)
#
# TIEMPO ESTIMADO DETALLADO:
# - ~27,540 imágenes de train (80% de 34,425)
# - Batch size 24 → ~1,148 batches/epoch (vs 2,295 con batch 12)
# - 100 epochs → ~114,800 batches totales
# - Con RTX 3090/4090: ~10-12 min/epoch → ~16-20 horas
# - Con A100: ~6-8 min/epoch → ~10-13 horas
# - Con V100 32GB: ~12-15 min/epoch → ~20-25 horas
#
# REQUISITOS:
# - VRAM: 18-20GB usado (batch 24 + mixed precision + modelo)
# - RAM: 24GB+ recomendado (num_workers 10 + dataset caching)
# - Disco: ~15GB (dataset + checkpoints + logs)
# - GPU: RTX 3090, RTX 4090, A5000, V100 32GB, A100, H100
#
# MONITOREO RECOMENDADO:
# - Epoch 1-5: Validar que no hay OOM
#   * Si OOM: reducir batch_size a 20 o num_workers a 8
# - Epoch 10-15: IoU debería estar >0.82
#   * Si <0.78: revisar datos o aumentar LR ligeramente
# - Epoch 30-40: IoU debería estar >0.85
# - Convergencia esperada: epoch 60-80
# - Early stopping: safety net (patience 20)
#
# 🎯 CUÁNDO USAR ESTA CONFIG:
# ✅ Primera vez entrenando con 20GB VRAM
# ✅ Quieres convergencia predecible y estable
# ✅ No tienes prisa (100 epochs completos)
# ✅ Prefieres seguridad sobre velocidad
#
# 🔄 SI ESTA CONFIG FUNCIONA BIEN:
# Considera probar: aisegment_full_optimized_20gb_aggressive.yaml
# - LR más alto (0.0003)
# - Converge ~15-20% más rápido
# - Requiere más monitoreo
#
# 📚 REFERENCIAS:
# - "Accurate, Large Minibatch SGD" (Goyal et al., 2017)
#   → Recomiendan √batch scaling para tareas visuales
# - "Don't Decay LR, Increase Batch Size" (Smith et al., 2018)
#   → Batch grande + LR escalado = mejor generalización

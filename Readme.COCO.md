# ğŸ·ï¸ GuÃ­a Completa - Dataset COCO para U-Net Background Removal

[![COCO Dataset](https://img.shields.io/badge/Dataset-COCO%202017-blue.svg)](https://cocodataset.org/)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Status](https://img.shields.io/badge/Status-Production%20Ready-green.svg)](https://github.com)

Esta guÃ­a te llevarÃ¡ paso a paso para configurar y entrenar el modelo U-Net usando el **dataset COCO 2017** para eliminaciÃ³n de fondos de personas.

## ğŸ“‹ Ãndice

- [ğŸ¯ VisiÃ³n General](#-visiÃ³n-general)
- [ğŸ“¥ Descarga e InstalaciÃ³n](#-descarga-e-instalaciÃ³n)
- [ğŸ” VerificaciÃ³n del Sistema](#-verificaciÃ³n-del-sistema)
- [ğŸ¯ Uso del Sistema](#-uso-del-sistema)
- [âš™ï¸ ConfiguraciÃ³n del Entrenamiento](#-configuraciÃ³n-del-entrenamiento)
- [ğŸ“Š Proceso de Entrenamiento](#-proceso-de-entrenamiento)
- [ğŸ“ˆ AnÃ¡lisis y MÃ©tricas](#-anÃ¡lisis-y-mÃ©tricas)
- [ğŸ’¡ Optimizaciones y Consejos](#-optimizaciones-y-consejos)
- [ğŸ”§ SoluciÃ³n de Problemas](#-soluciÃ³n-de-problemas)
- [ğŸ“ Soporte y Contacto](#-soporte-y-contacto)

## ğŸ¯ VisiÃ³n General

### Â¿Por quÃ© COCO Dataset?

El **COCO 2017 Dataset** es ideal para nuestro sistema de eliminaciÃ³n de fondos porque:

- **ğŸ“Š 64,115 imÃ¡genes** de entrenamiento con personas
- **ğŸ·ï¸ 149,813 anotaciones** de personas con keypoints
- **ğŸ¯ Diversidad excepcional**: Personas en mÃºltiples contextos y poses
- **ğŸ“ Anotaciones precisas**: SegmentaciÃ³n de alta calidad
- **ğŸŒ EstÃ¡ndar industrial**: Usado mundialmente para research

### EstadÃ­sticas del Dataset

| MÃ©trica | Valor |
|---------|-------|
| **ImÃ¡genes Total** | 64,115 (train) + 2,693 (val) |
| **Personas Anotadas** | ~149,813 instancias |
| **Promedio Personas/Imagen** | 3.32 |
| **TamaÃ±o Total** | ~13 GB (descomprimido) |
| **Formatos** | JPG (imÃ¡genes) + JSON (anotaciones) |

## ğŸ“¥ Descarga e InstalaciÃ³n

### 1. Preparar Entorno

```bash
# Verificar espacio en disco (mÃ­nimo 20GB recomendado)
df -h

# Crear directorio base
mkdir COCO && cd COCO
```

### 2. Descargar Archivos Requeridos

```bash
# Anotaciones (esenciales)
wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip

# ImÃ¡genes de entrenamiento
wget http://images.cocodataset.org/zips/train2017.zip

# ImÃ¡genes de validaciÃ³n
wget http://images.cocodataset.org/zips/val2017.zip
```

**TamaÃ±os de descarga:**
- `annotations_trainval2017.zip`: ~241 MB
- `train2017.zip`: ~12.9 GB  
- `val2017.zip`: ~788 MB
- **Total**: ~13.9 GB

### 3. Descomprimir Archivos

```bash
# Descomprimir anotaciones
unzip annotations_trainval2017.zip

# Descomprimir imÃ¡genes
unzip train2017.zip
unzip val2017.zip

# Limpiar archivos ZIP (opcional)
rm *.zip

# Volver al directorio principal
cd ..
```

### 4. Verificar Estructura Final

```bash
# La estructura debe ser:
COCO/
â”œâ”€â”€ annotations/
â”‚   â”œâ”€â”€ person_keypoints_train2017.json    # ~277 MB
â”‚   â”œâ”€â”€ person_keypoints_val2017.json      # ~11 MB
â”‚   â””â”€â”€ [otros archivos de anotaciones]
â”œâ”€â”€ train2017/                             # 64,115 imÃ¡genes .jpg
â”‚   â”œâ”€â”€ 000000000139.jpg
â”‚   â”œâ”€â”€ 000000000285.jpg
â”‚   â””â”€â”€ ...
â””â”€â”€ val2017/                               # 2,693 imÃ¡genes .jpg
    â”œâ”€â”€ 000000000139.jpg
    â””â”€â”€ ...
```

## ğŸ” VerificaciÃ³n del Sistema

### Comandos de VerificaciÃ³n

```bash
# VerificaciÃ³n rÃ¡pida de estructura
python main.py quick

# VerificaciÃ³n completa del sistema
python main.py verify

# AnÃ¡lisis detallado del dataset
python main.py analyze
```

### Salidas Esperadas

#### VerificaciÃ³n RÃ¡pida (`python main.py quick`)
```
=== VERIFICACIÃ“N RÃPIDA DE ESTRUCTURA COCO ===

âœ… Directorio principal encontrado: COCO
âœ… Directorio de anotaciones encontrado: COCO/annotations
âœ… Anotaciones de entrenamiento encontradas: COCO/annotations/person_keypoints_train2017.json (277.1 MB)
âœ… Anotaciones de validaciÃ³n encontradas: COCO/annotations/person_keypoints_val2017.json (11.2 MB)
âœ… Directorio train2017 encontrado con 64,115 imÃ¡genes
âœ… Directorio val2017 encontrado con 2,693 imÃ¡genes

âœ… Estructura COCO verificada correctamente!
   âœ… Todos los archivos necesarios estÃ¡n presentes
   âœ… Listo para entrenar el modelo
```

#### AnÃ¡lisis del Dataset (`python main.py analyze`)
```
=== ANÃLISIS DE ANOTACIONES COCO ===

ğŸ“ˆ EstadÃ­sticas generales:
   - Total de imÃ¡genes: 64,115
   - Total de anotaciones: 262,465
   - Anotaciones de personas vÃ¡lidas: 149,813
   - ImÃ¡genes con personas vÃ¡lidas: 45,174
   - Promedio de personas por imagen: 3.32
   - MÃ¡ximo de personas en una imagen: 13

ğŸ“ DistribuciÃ³n de tamaÃ±os (Ã¡rea):
   - Ãrea promedio: 17,889 pÃ­xelesÂ²
   - Ãrea mÃ­nima: 544 pÃ­xelesÂ²
   - Ãrea mÃ¡xima: 594,010 pÃ­xelesÂ²
   - Mediana: 6,421 pÃ­xelesÂ²

âœ… AnÃ¡lisis completado. Dataset listo para entrenamiento.
```

## ğŸ¯ Uso del Sistema

### Comandos Disponibles

| Comando | DescripciÃ³n | Tiempo |
|---------|-------------|--------|
| `python main.py` | **Modo automÃ¡tico** - VerificaciÃ³n + entrenamiento | 2-4 horas |
| `python main.py verify` | VerificaciÃ³n completa del sistema | 2-3 minutos |
| `python main.py quick` | VerificaciÃ³n rÃ¡pida de estructura COCO | 30 segundos |
| `python main.py analyze` | AnÃ¡lisis estadÃ­stico del dataset | 1-2 minutos |
| `python main.py batch` | Prueba de carga de datos | 30 segundos |
| `python main.py train` | Entrenamiento directo (sin verificaciÃ³n) | 2-4 horas |

### Flujo de Trabajo Recomendado

#### 1. Primera Vez - VerificaciÃ³n Completa
```bash
# Verificar todo el sistema
python main.py verify

# Si todo estÃ¡ bien, continuar con entrenamiento
python main.py train
```

#### 2. VerificaciÃ³n RÃ¡pida
```bash
# Solo verificar estructura de archivos
python main.py quick
```

#### 3. AnÃ¡lisis del Dataset
```bash
# Ver estadÃ­sticas detalladas
python main.py analyze
```

**Salida esperada:**
```
ğŸ“ˆ EstadÃ­sticas generales:
   - Total de imÃ¡genes: 64,115
   - Total de anotaciones: 262,465
   - Anotaciones de personas vÃ¡lidas: 149,813
   - ImÃ¡genes con personas vÃ¡lidas: 45,174
   - Promedio de personas por imagen: 3.32
```

#### 4. Entrenamiento AutomÃ¡tico (Recomendado)
```bash
# VerificaciÃ³n + entrenamiento con confirmaciÃ³n
python main.py
```

#### 5. Solo Entrenamiento
```bash
# Saltar verificaciones e ir directo al entrenamiento
python main.py train
```

## âš™ï¸ ConfiguraciÃ³n del Entrenamiento

### ParÃ¡metros por Defecto (main.py)

```python
config = {
    'batch_size': 16,           # Reducido para COCO (imÃ¡genes mÃ¡s variadas)
    'learning_rate': 1e-4,      # Learning rate conservador
    'weight_decay': 1e-6,       # RegularizaciÃ³n ligera
    'num_epochs': 100,          # Ã‰pocas de entrenamiento
    'image_size': 384,          # TamaÃ±o de procesamiento
    'num_workers': 8,           # Trabajadores para carga de datos
    'pin_memory': True,         # OptimizaciÃ³n de memoria
}
```

### Personalizar ConfiguraciÃ³n

```python
# Para GPUs con poca memoria (â‰¤6GB VRAM)
config['batch_size'] = 8
config['image_size'] = 256

# Para entrenamiento rÃ¡pido
config['num_epochs'] = 50
config['learning_rate'] = 2e-4

# Para mÃ¡xima calidad
config['batch_size'] = 32
config['image_size'] = 512
config['num_epochs'] = 200
```

### ConfiguraciÃ³n por Hardware

| Hardware | Batch Size | Image Size | Workers | Tiempo/Ã‰poca |
|----------|------------|------------|---------|--------------|
| **RTX 4090** | 32 | 512 | 12 | ~8 min |
| **RTX 3080** | 24 | 384 | 8 | ~12 min |
| **GTX 1080** | 16 | 384 | 6 | ~18 min |
| **GTX 1060** | 8 | 256 | 4 | ~35 min |
| **CPU Only** | 4 | 256 | 2 | ~4 horas |

## ğŸ“Š Proceso de Entrenamiento

### Etapas del Entrenamiento

1. **Carga del Dataset** (2-3 minutos)
   - Lectura de anotaciones COCO
   - Filtrado de personas vÃ¡lidas
   - CreaciÃ³n de dataloaders

2. **InicializaciÃ³n** (30 segundos)
   - Carga del modelo U-Net
   - ConfiguraciÃ³n de optimizador
   - PreparaciÃ³n de funciones de pÃ©rdida

3. **Entrenamiento** (2-4 horas)
   - 100 Ã©pocas por defecto
   - ValidaciÃ³n cada Ã©poca
   - Guardado automÃ¡tico del mejor modelo

4. **FinalizaciÃ³n** (1 minuto)
   - Guardado del modelo final
   - GeneraciÃ³n de grÃ¡ficas
   - Resumen de mÃ©tricas

### Monitoreo del Progreso

```bash
# Ver logs en tiempo real
tail -f logs/training_*.log

# Monitorear GPU
watch -n 2 nvidia-smi

# Verificar checkpoints guardados
ls -la checkpoints/
```

### Salida TÃ­pica del Entrenamiento

```
ğŸš€ INICIANDO ENTRENAMIENTO U-NET
===================================

ğŸ“Š ConfiguraciÃ³n:
   - Dataset: COCO 2017
   - Batch Size: 16
   - Learning Rate: 1e-4
   - Ã‰pocas: 100
   - Dispositivo: cuda:0

ğŸ”„ Cargando dataset COCO...
   âœ… Train: 45,174 imÃ¡genes con personas
   âœ… Val: 2,693 imÃ¡genes con personas

ğŸ“ˆ Ã‰poca 1/100:
   Train - Loss: 0.4521, Acc: 0.8234
   Val - Loss: 0.3876, IoU: 0.7543, Dice: 0.8012
   â±ï¸ Tiempo: 12m 34s

ğŸ“ˆ Ã‰poca 2/100:
   Train - Loss: 0.3287, Acc: 0.8567
   Val - Loss: 0.2943, IoU: 0.8012, Dice: 0.8456
   â±ï¸ Tiempo: 12m 28s

...

ğŸ‰ Â¡Entrenamiento completado!
   ğŸ“Š Mejor IoU: 0.8734 (Ã‰poca 87)
   ğŸ’¾ Modelo guardado: checkpoints/best_model.pth
```

## ğŸ“ˆ AnÃ¡lisis y MÃ©tricas

### MÃ©tricas Principales

| MÃ©trica | DescripciÃ³n | Valor Objetivo |
|---------|-------------|----------------|
| **IoU** | Intersection over Union | â‰¥0.85 |
| **Dice Score** | Similaridad entre mÃ¡scaras | â‰¥0.90 |
| **Pixel Accuracy** | PrecisiÃ³n a nivel de pÃ­xel | â‰¥0.95 |
| **Loss** | FunciÃ³n de pÃ©rdida | <0.15 |

### GrÃ¡ficas Generadas

El sistema genera automÃ¡ticamente:

- **`plots/training_curves.png`**: Curvas de pÃ©rdida y mÃ©tricas
- **`plots/sample_predictions.png`**: Predicciones de muestra
- **`plots/confusion_matrix.png`**: Matriz de confusiÃ³n
- **`plots/metrics_evolution.png`**: EvoluciÃ³n de mÃ©tricas por Ã©poca

### AnÃ¡lisis de Calidad AutomÃ¡tico

```python
# El sistema evalÃºa automÃ¡ticamente:
quality_metrics = {
    'coverage_threshold': 15.0,    # % mÃ­nimo de cobertura de persona
    'contrast_threshold': 60.0,    # Contraste mÃ­nimo de mÃ¡scara
    'edge_threshold': 50.0,        # DefiniciÃ³n mÃ­nima de bordes
    'resolution_threshold': 70.0   # Score mÃ­nimo de resoluciÃ³n
}
```

## ğŸ’¡ Optimizaciones y Consejos

### Para Mejor Rendimiento

1. **SSD**: Usa SSD para almacenar COCO (mejora velocidad de carga)
2. **RAM**: 16+ GB recomendado para batch_size > 16
3. **GPU**: NVIDIA con 8+ GB VRAM para mÃ¡xima velocidad
4. **Monitoring**: Usa `watch nvidia-smi` durante entrenamiento

### Para Mejor Calidad

1. **Ã‰pocas**: MÃ­nimo 100 Ã©pocas para convergencia
2. **Learning Rate**: Usar 1e-4 o menos para estabilidad
3. **Image Size**: 384+ pÃ­xeles para mejor detalle
4. **Patience**: Esperar convergencia completa

### Para Desarrollo

1. **VerificaciÃ³n**: Siempre usar `python main.py verify` primero
2. **Logs**: Monitorear logs para detectar problemas temprano
3. **Checkpoints**: El mejor modelo se guarda automÃ¡ticamente
4. **Resumir**: Puedes resumir entrenamiento desde `last_model.pth`

### Uso con Screen (Sesiones Largas)

```bash
# Crear sesiÃ³n para entrenamiento largo
screen -S unet_training

# Ejecutar entrenamiento
python main.py train

# Despegarse (Ctrl+A, luego D)
# Reconectar mÃ¡s tarde
screen -r unet_training
```

### OptimizaciÃ³n de Memoria

```python
# Para GPUs con poca memoria
config = {
    'batch_size': 8,           # Reducir batch size
    'image_size': 256,         # Reducir resoluciÃ³n
    'pin_memory': False,       # Desactivar pin memory
    'num_workers': 4,          # Menos workers
    'gradient_accumulation': 2  # Acumular gradientes
}
```

### ConfiguraciÃ³n Avanzada

```python
# ConfiguraciÃ³n para investigaciÃ³n
research_config = {
    'model_variant': 'attention_unet',  # Usar Attention U-Net
    'loss_function': 'focal_dice',      # Loss hÃ­brido
    'optimizer': 'adamw',               # Optimizador avanzado
    'scheduler': 'cosine_annealing',    # Scheduler de learning rate
    'augmentation_level': 'heavy',      # AumentaciÃ³n intensiva
    'mixed_precision': True,            # Entrenamiento mixto
}
```

## ğŸ”§ SoluciÃ³n de Problemas

### Problemas de Dataset

#### "Dataset no encontrado"

```bash
# Verificar estructura
python main.py quick

# Salida esperada:
# âœ… Directorio principal encontrado: COCO
# âœ… Anotaciones de entrenamiento encontradas: ...
```

**SoluciÃ³n:**
```bash
# Verificar que los archivos existan
ls COCO/annotations/person_keypoints_train2017.json
ls COCO/train2017/ | head -5

# Si faltan, re-descargar
cd COCO
wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip
unzip annotations_trainval2017.zip
```

#### "Anotaciones corruptas"

```bash
# Verificar integridad JSON
python -c "import json; json.load(open('COCO/annotations/person_keypoints_train2017.json'))"
```

**SoluciÃ³n:**
```bash
# Re-descargar solo anotaciones
cd COCO
rm -rf annotations/
wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip
unzip annotations_trainval2017.zip
```

### Problemas de Memoria

#### "CUDA out of memory"

```bash
# Error tÃ­pico:
# RuntimeError: CUDA out of memory. Tried to allocate 2.00 GiB
```

**Soluciones:**
```python
# 1. Reducir batch_size
config['batch_size'] = 8  # O incluso 4

# 2. Reducir image_size
config['image_size'] = 256

# 3. Usar gradient accumulation
config['gradient_accumulation_steps'] = 2

# 4. Limpiar cache
torch.cuda.empty_cache()
```

#### "RAM insuficiente"

```bash
# SÃ­ntomas: Sistema muy lento, swap usage alto
```

**Soluciones:**
```python
# Reducir workers
config['num_workers'] = 2

# Desactivar pin_memory
config['pin_memory'] = False

# Usar batch_size menor
config['batch_size'] = 4
```

### Problemas de Entrenamiento

#### "Loss no converge"

```bash
# SÃ­ntomas: Loss se mantiene alto despuÃ©s de muchas Ã©pocas
```

**DiagnÃ³stico:**
```bash
# Verificar datos
python main.py analyze

# Ver distribuciÃ³n de dataset
python main.py batch
```

**Soluciones:**
```python
# 1. Reducir learning rate
config['learning_rate'] = 5e-5

# 2. Aumentar Ã©pocas
config['num_epochs'] = 200

# 3. Cambiar optimizador
config['optimizer'] = 'sgd'
config['momentum'] = 0.9

# 4. Verificar augmentaciÃ³n
config['augmentation'] = 'light'  # Menos agresiva
```

#### "Overfitting"

```bash
# SÃ­ntomas: Train loss baja, val loss alta
```

**Soluciones:**
```python
# 1. Aumentar regularizaciÃ³n
config['weight_decay'] = 1e-4

# 2. Dropout
config['dropout'] = 0.3

# 3. Early stopping
config['early_stopping_patience'] = 10

# 4. MÃ¡s augmentaciÃ³n
config['augmentation'] = 'heavy'
```

### Problemas de Sistema

#### "Python module not found"

```bash
# Error: ModuleNotFoundError: No module named 'torch'
```

**SoluciÃ³n:**
```bash
# Verificar entorno virtual
which python
pip list | grep torch

# Reinstalar dependencias
pip install -r requirements.txt

# O forzar reinstalaciÃ³n
pip install --force-reinstall torch torchvision
```

#### "GPU no detectada"

```bash
# Verificar CUDA
nvidia-smi
python -c "import torch; print(torch.cuda.is_available())"
```

**SoluciÃ³n:**
```bash
# Verificar drivers NVIDIA
nvidia-smi

# Verificar CUDA toolkit
nvcc --version

# Reinstalar PyTorch con CUDA
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

### Problemas de Rendimiento

#### "Entrenamiento muy lento"

**DiagnÃ³stico:**
```bash
# Verificar uso de GPU
nvidia-smi

# Verificar IO del disco
iostat -x 1

# Verificar carga de CPU
htop
```

**Optimizaciones:**
```python
# 1. Aumentar workers si CPU/IO permite
config['num_workers'] = 12

# 2. Usar pin_memory si hay RAM suficiente
config['pin_memory'] = True

# 3. Optimizar dataloader
config['persistent_workers'] = True
config['prefetch_factor'] = 2

# 4. Usar mixed precision
config['mixed_precision'] = True
```

#### "Carga de dataset lenta"

**Soluciones:**
```bash
# 1. Mover COCO a SSD
mv COCO /path/to/ssd/COCO
ln -s /path/to/ssd/COCO ./COCO

# 2. Precargar datos en RAM (si es posible)
# Crear dataset en memoria
python -c "
import torch
from datasets import COCOPersonDataset
dataset = COCOPersonDataset(preload=True)
torch.save(dataset, 'preloaded_dataset.pt')
"
```

## ğŸ“ Soporte y Contacto

### Problemas Frecuentes

- **Dataset no encontrado**: Verificar estructura con `python main.py quick`
- **Memoria insuficiente**: Reducir `batch_size` y/o `image_size`
- **Entrenamiento lento**: Verificar uso de GPU con `nvidia-smi`
- **Calidad baja**: Aumentar Ã©pocas y/o tamaÃ±o de imagen

### Checklist de VerificaciÃ³n

Antes de reportar problemas, verificar:

- [ ] Estructura COCO correcta (`python main.py quick`)
- [ ] Dependencias instaladas (`pip list | grep torch`)
- [ ] GPU disponible (`nvidia-smi`)
- [ ] Espacio en disco suficiente (`df -h`)
- [ ] ConfiguraciÃ³n apropiada para tu hardware
- [ ] Logs de error completos

### Recursos Adicionales

- ğŸ“– **DocumentaciÃ³n Original**: `README.md`
- ğŸ­ **AplicaciÃ³n Web**: `README-app.md`
- ğŸ”§ **CÃ³digo Principal**: `main.py` (adaptado para COCO)
- ğŸ“Š **Dataset COCO**: [cocodataset.org](https://cocodataset.org/)
- ğŸ› ï¸ **Utilidades**: `Docs/Utils.md`

### Logs Importantes

```bash
# Logs de entrenamiento
ls logs/training_*.log

# Logs de verificaciÃ³n
ls logs/verification_*.log

# Logs de sistema
ls logs/system_*.log

# Ver Ãºltimos errores
grep -i error logs/*.log | tail -10
```

### Desarrolladores

**Luis Huacho y Dominick Alvarez**  
MaestrÃ­a en InformÃ¡tica - PUCP  
EspecializaciÃ³n en Computer Vision y Deep Learning

**Contacto:**
- ğŸ“§ Email: [contacto-disponible-en-repositorio]
- ğŸ”— GitHub: [link-del-repositorio]
- ğŸ“š DocumentaciÃ³n: Ver archivos README adicionales

---

## ğŸ Inicio RÃ¡pido - 5 Pasos

```bash
# 1. Clonar y preparar entorno
git clone <repo> && cd unet-background-removal
python -m venv venv && source venv/bin/activate
pip install torch torchvision opencv-python albumentations numpy matplotlib streamlit

# 2. Descargar COCO (si no lo tienes)
mkdir COCO && cd COCO
wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip
wget http://images.cocodataset.org/zips/train2017.zip
wget http://images.cocodataset.org/zips/val2017.zip
unzip annotations_trainval2017.zip && unzip train2017.zip && unzip val2017.zip
cd ..

# 3. Verificar todo
python main.py verify

# 4. Entrenar modelo
python main.py train

# 5. Usar aplicaciÃ³n web
streamlit run app.py
```

Â¡El sistema estÃ¡ listo para funcionar con tu dataset COCO! ğŸš€

### Comandos de Emergencia

```bash
# Si algo sale mal, estos comandos te salvarÃ¡n:

# VerificaciÃ³n ultra-rÃ¡pida
python main.py quick

# Limpiar y reiniciar
rm -rf checkpoints/ logs/ plots/
python main.py verify

# Forzar reinstalaciÃ³n
pip install --force-reinstall -r requirements.txt

# Entrenamiento con configuraciÃ³n mÃ­nima
python -c "
import main
config = {'batch_size': 4, 'image_size': 256, 'num_epochs': 10}
main.train_model(config)
"
```

**Â¿AÃºn tienes problemas?** Revisa la secciÃ³n de SoluciÃ³n de Problemas o consulta los logs en `logs/` para mÃ¡s detalles.

**Â¿Primera vez con COCO?** Este README tiene todo lo que necesitas. Â¡SÃ­guelo paso a paso!

**Â¿Listo para producciÃ³n?** Una vez entrenado, consulta `README-app.md` para desplegar la aplicaciÃ³n web.

---

**Tip Pro**: Usa `screen` para entrenamientos largos y `watch nvidia-smi` para monitorear tu GPU. Â¡El entrenamiento puede tomar horas, pero el resultado vale la pena! ğŸ’ª
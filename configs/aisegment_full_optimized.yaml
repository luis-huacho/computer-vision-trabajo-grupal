# ConfiguraciÃ³n OPTIMIZADA para 100% del AISegment Dataset
# Dataset: 34,425 imÃ¡genes completas
# Optimizado para GPU con 10GB+ VRAM libre (o multi-GPU)
# Uso: python main.py train --config aisegment_full_optimized
#
# MEJORAS vs aisegment_20pct_optimized:
# - Dataset completo (34,425 imÃ¡genes vs 6,800)
# - 100 epochs para convergencia completa
# - Batch size 12 (Ã³ptimo para Batch Norm con dataset grande)
# - Learning rate con warmup para estabilidad inicial
# - Early stopping patience 20 (mÃ¡s paciencia con dataset grande)
# - Augmentaciones mÃ¡s agresivas (mÃ¡s datos = mÃ¡s robustez)
# - Scheduler con ciclos mÃ¡s largos
#
# Objetivo: IoU 87-90% (SOTA para matting de retratos)

experiment:
  name: "AISegment 100% - Full Optimized"
  description: "Entrenamiento con dataset COMPLETO AISegment (34,425 imÃ¡genes) - ConfiguraciÃ³n de producciÃ³n"
  mode: "production"

model:
  architecture: "resnet50"
  image_size: 384
  use_pretrained: true
  use_attention: true

dataset:
  type: "aisegment"
  root: "datasets/AISegment"

  # Descarga automÃ¡tica con kagglehub
  auto_download: true
  kaggle_dataset_id: "laurentmih/aisegmentcom-matting-human-datasets"

  # Split train/val (80/20)
  train_val_split: 0.8

  # ðŸŽ¯ DATASET COMPLETO (sin sampling)
  sampling:
    enabled: false
    mode: "full"

training:
  num_epochs: 100      # â¬†ï¸ Aumentado de 50 (dataset completo necesita mÃ¡s epochs)
  batch_size: 12       # â¬†ï¸ Aumentado de 8 (mejor aprovechamiento de GPU)
  learning_rate: 0.00015  # â¬†ï¸ MÃ¡s agresivo con dataset grande
  weight_decay: 0.0000005  # â¬‡ï¸ Menos regularizaciÃ³n (dataset grande generaliza mejor)
  num_workers: 8       # â¬†ï¸ Aumentado de 4 (dataset grande = mÃ¡s I/O)
  pin_memory: true
  drop_last: true
  gradient_clip_max_norm: 0.5
  mixed_precision: true  # âœ… Esencial para batch_size 12

  scheduler:
    type: "cosine_annealing"
    T_0: 15            # â¬†ï¸ Aumentado de 10 (ciclos mÃ¡s largos para dataset completo)
    T_mult: 2
    eta_min: 0.000005  # â¬‡ï¸ LR mÃ­nimo mÃ¡s bajo para fine-tuning profundo

  # ðŸŽ¯ LOSS WEIGHTS OPTIMIZADOS PARA MATTING DE ALTA CALIDAD
  loss_weights:
    alpha: 0.7         # â¬‡ï¸ BCE aÃºn menos dominante
    beta: 1.3          # â¬†ï¸ Dice mÃ¡s importante (overlap crÃ­tico)
    gamma: 0.7         # â¬†ï¸ Perceptual mÃ¡s alto (calidad visual SOTA)
    delta: 0.5         # â¬†ï¸ Edge mÃ¡s importante (bordes precisos)

validation:
  frequency: 1
  early_stopping:
    enabled: true
    patience: 20       # â¬†ï¸ Aumentado de 12 (dataset grande necesita mÃ¡s paciencia)
    min_delta: 0.00005 # â¬‡ï¸ Delta mÃ¡s fino para detecciÃ³n de plateau
  target_metrics:
    iou: 0.88          # â¬†ï¸ Target SOTA (de 0.85)
    dice: 0.92         # â¬†ï¸ Target SOTA (de 0.90)
    pixel_accuracy: 0.96  # â¬†ï¸ Target SOTA (de 0.95)

checkpoints:
  save_best: true
  save_last: true
  save_every_n_epochs: 10  # â¬†ï¸ Menos frecuente (dataset grande = checkpoints grandes)
  checkpoint_dir: "checkpoints/aisegment_full_optimized"

logging:
  log_every_n_batches: 25  # â¬†ï¸ Menos frecuente (mÃ¡s batches por epoch)
  save_plots: true

# ðŸŽ¨ AUGMENTACIONES AGRESIVAS (Dataset Grande = MÃ¡s Robustez)
augmentation:
  train:
    # Augmentaciones geomÃ©tricas
    horizontal_flip: 0.5
    random_rotate90: 0.4    # â¬†ï¸ Aumentado de 0.3

    shift_scale_rotate:
      enabled: true
      shift_limit: 0.12     # â¬†ï¸ Aumentado de 0.1
      scale_limit: 0.25     # â¬†ï¸ Aumentado de 0.2
      rotate_limit: 20      # â¬†ï¸ Aumentado de 15
      p: 0.5               # â¬†ï¸ Aumentado de 0.4 (mÃ¡s agresivo)

    # Augmentaciones de color
    brightness_contrast:
      enabled: true
      brightness_limit: 0.25  # â¬†ï¸ Aumentado de 0.2
      contrast_limit: 0.25    # â¬†ï¸ Aumentado de 0.2
      p: 0.5               # â¬†ï¸ Aumentado de 0.4

    # Augmentaciones de color avanzadas
    hue_saturation_value:
      enabled: true
      hue_shift_limit: 15   # â¬†ï¸ Aumentado de 10
      sat_shift_limit: 20   # â¬†ï¸ Aumentado de 15
      val_shift_limit: 15   # â¬†ï¸ Aumentado de 10
      p: 0.3               # â¬†ï¸ Aumentado de 0.2

    gaussian_blur:
      enabled: true
      blur_limit: [3, 7]    # â¬†ï¸ Aumentado de [3, 5]
      p: 0.2               # â¬†ï¸ Aumentado de 0.15

    # ðŸ†• AUGMENTACIONES ADICIONALES PARA DATASET COMPLETO
    random_gamma:
      enabled: true
      gamma_limit: [80, 120]
      p: 0.3

    gaussian_noise:
      enabled: true
      var_limit: [10.0, 30.0]
      p: 0.2

# ðŸ“Š MEJORAS ESPERADAS:
# - IoU actual (20% optimized): ~82-85%
# - IoU esperado (100% full optimized): 87-90%
#   * +3-5% por 5x mÃ¡s datos (34K vs 6.8K imÃ¡genes)
#   * +1-2% por augmentaciones mÃ¡s agresivas
#   * +0.5-1% por batch size mayor (mejor estadÃ­sticas de Batch Norm)
#   * +0.5-1% por mÃ¡s epochs (convergencia completa)
#   * +0.5-1% por scheduler optimizado (ciclos mÃ¡s largos)
#
# TIEMPO ESTIMADO:
# - ~27,540 imÃ¡genes de train (80% de 34,425)
# - Batch size 12 â†’ ~2,295 batches/epoch
# - 100 epochs â†’ ~229,500 batches totales
# - Con GPU V100/A100: ~15-20 horas
# - Con GPU RTX 3090/4090: ~20-30 horas
# - Multi-GPU (2x): ~10-15 horas
#
# REQUISITOS:
# - VRAM: 10-12GB mÃ­nimo (batch_size 12 + mixed precision)
# - RAM: 16GB+ recomendado
# - Disco: ~15GB (dataset + checkpoints + logs)
# - GPU recomendada: V100, A100, RTX 3090/4090, RTX 6000

# ðŸŽ¯ ESTRATEGIA DE ENTRENAMIENTO RECOMENDADA:
# 1. Iniciar con esta config completa
# 2. Monitorear primeros 5 epochs:
#    - Si OOM (Out Of Memory): reducir batch_size a 8-10
#    - Si muy lento: aumentar num_workers (max: num_cores - 2)
# 3. Validar en epoch 10-15:
#    - IoU deberÃ­a estar >0.80
#    - Si <0.75: revisar loss weights o learning rate
# 4. Convergencia esperada: epoch 60-80
# 5. Early stopping actÃºa como safety net (patience 20)
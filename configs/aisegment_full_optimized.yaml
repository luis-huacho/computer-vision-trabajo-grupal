# Configuración OPTIMIZADA para 100% del AISegment Dataset
# Dataset: 34,425 imágenes completas
# Optimizado para GPU con 10GB+ VRAM libre (o multi-GPU)
# Uso: python main.py train --config aisegment_full_optimized
#
# MEJORAS vs aisegment_20pct_optimized:
# - Dataset completo (34,425 imágenes vs 6,800)
# - 100 epochs para convergencia completa
# - Batch size 12 (óptimo para Batch Norm con dataset grande)
# - Learning rate con warmup para estabilidad inicial
# - Early stopping patience 20 (más paciencia con dataset grande)
# - Augmentaciones más agresivas (más datos = más robustez)
# - Scheduler con ciclos más largos
#
# Objetivo: IoU 87-90% (SOTA para matting de retratos)

experiment:
  name: "AISegment 100% - Full Optimized"
  description: "Entrenamiento con dataset COMPLETO AISegment (34,425 imágenes) - Configuración de producción"
  mode: "production"

model:
  architecture: "resnet50"
  image_size: 384
  use_pretrained: true
  use_attention: true

dataset:
  type: "aisegment"
  root: "datasets/AISegment"

  # Descarga automática con kagglehub
  auto_download: true
  kaggle_dataset_id: "laurentmih/aisegmentcom-matting-human-datasets"

  # Split train/val (80/20)
  train_val_split: 0.8

  # 🎯 DATASET COMPLETO (sin sampling)
  sampling:
    enabled: false
    mode: "full"

training:
  num_epochs: 100      # ⬆️ Aumentado de 50 (dataset completo necesita más epochs)
  batch_size: 12       # ⬆️ Aumentado de 8 (mejor aprovechamiento de GPU)
  learning_rate: 0.00015  # ⬆️ Más agresivo con dataset grande
  weight_decay: 0.0000005  # ⬇️ Menos regularización (dataset grande generaliza mejor)
  num_workers: 8       # ⬆️ Aumentado de 4 (dataset grande = más I/O)
  pin_memory: true
  drop_last: true
  gradient_clip_max_norm: 0.5
  mixed_precision: true  # ✅ Esencial para batch_size 12

  scheduler:
    type: "cosine_annealing"
    T_0: 15            # ⬆️ Aumentado de 10 (ciclos más largos para dataset completo)
    T_mult: 2
    eta_min: 0.000005  # ⬇️ LR mínimo más bajo para fine-tuning profundo

  # 🎯 LOSS WEIGHTS OPTIMIZADOS PARA MATTING DE ALTA CALIDAD
  loss_weights:
    alpha: 0.7         # ⬇️ BCE aún menos dominante
    beta: 1.3          # ⬆️ Dice más importante (overlap crítico)
    gamma: 0.7         # ⬆️ Perceptual más alto (calidad visual SOTA)
    delta: 0.5         # ⬆️ Edge más importante (bordes precisos)

validation:
  frequency: 1
  early_stopping:
    enabled: true
    patience: 20       # ⬆️ Aumentado de 12 (dataset grande necesita más paciencia)
    min_delta: 0.00005 # ⬇️ Delta más fino para detección de plateau
  target_metrics:
    iou: 0.88          # ⬆️ Target SOTA (de 0.85)
    dice: 0.92         # ⬆️ Target SOTA (de 0.90)
    pixel_accuracy: 0.96  # ⬆️ Target SOTA (de 0.95)

checkpoints:
  save_best: true
  save_last: true
  save_every_n_epochs: 10  # ⬆️ Menos frecuente (dataset grande = checkpoints grandes)
  checkpoint_dir: "checkpoints/aisegment_full_optimized"

logging:
  log_every_n_batches: 25  # ⬆️ Menos frecuente (más batches por epoch)
  save_plots: true

# 🎨 AUGMENTACIONES AGRESIVAS (Dataset Grande = Más Robustez)
augmentation:
  train:
    # Augmentaciones geométricas
    horizontal_flip: 0.5
    random_rotate90: 0.4    # ⬆️ Aumentado de 0.3

    shift_scale_rotate:
      enabled: true
      shift_limit: 0.12     # ⬆️ Aumentado de 0.1
      scale_limit: 0.25     # ⬆️ Aumentado de 0.2
      rotate_limit: 20      # ⬆️ Aumentado de 15
      p: 0.5               # ⬆️ Aumentado de 0.4 (más agresivo)

    # Augmentaciones de color
    brightness_contrast:
      enabled: true
      brightness_limit: 0.25  # ⬆️ Aumentado de 0.2
      contrast_limit: 0.25    # ⬆️ Aumentado de 0.2
      p: 0.5               # ⬆️ Aumentado de 0.4

    # Augmentaciones de color avanzadas
    hue_saturation_value:
      enabled: true
      hue_shift_limit: 15   # ⬆️ Aumentado de 10
      sat_shift_limit: 20   # ⬆️ Aumentado de 15
      val_shift_limit: 15   # ⬆️ Aumentado de 10
      p: 0.3               # ⬆️ Aumentado de 0.2

    gaussian_blur:
      enabled: true
      blur_limit: [3, 7]    # ⬆️ Aumentado de [3, 5]
      p: 0.2               # ⬆️ Aumentado de 0.15

    # 🆕 AUGMENTACIONES ADICIONALES PARA DATASET COMPLETO
    random_gamma:
      enabled: true
      gamma_limit: [80, 120]
      p: 0.3

    gaussian_noise:
      enabled: true
      var_limit: [10.0, 30.0]
      p: 0.2

# 📊 MEJORAS ESPERADAS:
# - IoU actual (20% optimized): ~82-85%
# - IoU esperado (100% full optimized): 87-90%
#   * +3-5% por 5x más datos (34K vs 6.8K imágenes)
#   * +1-2% por augmentaciones más agresivas
#   * +0.5-1% por batch size mayor (mejor estadísticas de Batch Norm)
#   * +0.5-1% por más epochs (convergencia completa)
#   * +0.5-1% por scheduler optimizado (ciclos más largos)
#
# TIEMPO ESTIMADO:
# - ~27,540 imágenes de train (80% de 34,425)
# - Batch size 12 → ~2,295 batches/epoch
# - 100 epochs → ~229,500 batches totales
# - Con GPU V100/A100: ~15-20 horas
# - Con GPU RTX 3090/4090: ~20-30 horas
# - Multi-GPU (2x): ~10-15 horas
#
# REQUISITOS:
# - VRAM: 10-12GB mínimo (batch_size 12 + mixed precision)
# - RAM: 16GB+ recomendado
# - Disco: ~15GB (dataset + checkpoints + logs)
# - GPU recomendada: V100, A100, RTX 3090/4090, RTX 6000

# 🎯 ESTRATEGIA DE ENTRENAMIENTO RECOMENDADA:
# 1. Iniciar con esta config completa
# 2. Monitorear primeros 5 epochs:
#    - Si OOM (Out Of Memory): reducir batch_size a 8-10
#    - Si muy lento: aumentar num_workers (max: num_cores - 2)
# 3. Validar en epoch 10-15:
#    - IoU debería estar >0.80
#    - Si <0.75: revisar loss weights o learning rate
# 4. Convergencia esperada: epoch 60-80
# 5. Early stopping actúa como safety net (patience 20)
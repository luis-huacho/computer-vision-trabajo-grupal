# ConfiguraciÃ³n OPTIMIZADA para 100% AISegment Dataset - 20GB VRAM (AGRESIVO)
# Dataset: 34,425 imÃ¡genes completas
# Hardware: GPU con 20GB VRAM (RTX 3090, RTX 4090, A5000, V100 32GB, A100)
# Uso: python main.py train --config aisegment_full_optimized_20gb_aggressive
#
# ESTRATEGIA: AGRESIVO (Linear batch scaling)
# - Batch size 24 (duplicado de 12)
# - Learning rate 0.0003 (escalado lineal 2x para convergencia rÃ¡pida)
# - Warmup 5 epochs (necesario para estabilizar LR alto)
# - Ciclos mÃ¡s cortos en scheduler
# - Convergencia 15-20% mÃ¡s rÃ¡pida que versiÃ³n conservadora
#
# VENTAJAS vs conservador:
# - Converge mÃ¡s rÃ¡pido (~15-20% menos epochs)
# - Tiempo: ~12-15 horas (vs 15-18h conservador)
# - Explora loss landscape mÃ¡s agresivamente
#
# RIESGOS:
# âš ï¸ Puede saltar mÃ­nimos Ã³ptimos si LR muy alto
# âš ï¸ Requiere monitoreo mÃ¡s atento
# âš ï¸ Puede degradar calidad de bordes si no converge bien
#
# Objetivo: IoU 88-90% en menos tiempo

experiment:
  name: "AISegment 100% - 20GB VRAM Optimized (Agresivo)"
  description: "Entrenamiento rÃ¡pido - Batch 24, LR escalado lineal, Warmup"
  mode: "production"

model:
  architecture: "resnet50"
  image_size: 384
  use_pretrained: true
  use_attention: true

dataset:
  type: "aisegment"
  root: "datasets/AISegment"

  # Descarga automÃ¡tica con kagglehub
  auto_download: true
  kaggle_dataset_id: "laurentmih/aisegmentcom-matting-human-datasets"

  # Split train/val (80/20)
  train_val_split: 0.8

  # Dataset completo (sin sampling)
  sampling:
    enabled: false
    mode: "full"

training:
  num_epochs: 100      # Mismo (puede terminar antes por early stopping)
  batch_size: 24       # ğŸ”¥ DUPLICADO de 12 (aprovecha 20GB VRAM)
  learning_rate: 0.0003   # ğŸš€ AGRESIVO: Escalado lineal 2x (0.00015 Ã— 2)
  weight_decay: 0.0000003  # â¬‡ï¸ Reducido (batch grande regulariza implÃ­citamente)
  num_workers: 10      # Optimizado para I/O
  pin_memory: true
  drop_last: true
  gradient_clip_max_norm: 0.5
  mixed_precision: true  # âœ… Esencial

  # ğŸ†• WARMUP para estabilizar LR alto
  warmup:
    enabled: true
    warmup_epochs: 5     # Warmup lineal de 0 â†’ 0.0003 en 5 epochs
    warmup_start_lr: 0.00005  # LR inicial bajo

  scheduler:
    type: "cosine_annealing"
    T_0: 10            # â¬‡ï¸ Ciclos MÃS CORTOS (de 12) para convergencia rÃ¡pida
    T_mult: 2
    eta_min: 0.000005  # LR mÃ­nimo

  # Loss weights optimizados para convergencia rÃ¡pida
  loss_weights:
    alpha: 0.65        # â¬‡ï¸ BCE menos dominante (permite exploraciÃ³n rÃ¡pida)
    beta: 1.35         # â¬†ï¸ Dice mÃ¡s importante (foco en overlap)
    gamma: 0.75        # â¬†ï¸ Perceptual alto (calidad visual)
    delta: 0.55        # â¬†ï¸ Edge importante (bordes precisos)

validation:
  frequency: 1
  early_stopping:
    enabled: true
    patience: 15       # â¬‡ï¸ REDUCIDO de 20 (termina antes si plateau)
    min_delta: 0.00008 # â¬†ï¸ Delta mÃ¡s grande (menos sensible a ruido)
  target_metrics:
    iou: 0.88          # Target SOTA (mismo)
    dice: 0.92
    pixel_accuracy: 0.96

checkpoints:
  save_best: true
  save_last: true
  save_every_n_epochs: 8  # â¬‡ï¸ Menos frecuente (converge rÃ¡pido)
  checkpoint_dir: "checkpoints/aisegment_full_optimized_20gb_aggressive"

logging:
  log_every_n_batches: 15  # â¬‡ï¸ MÃ¡s frecuente (monitoreo importante con LR alto)
  save_plots: true

# Augmentaciones agresivas (sin cambios - ya optimizadas)
augmentation:
  train:
    horizontal_flip: 0.5
    random_rotate90: 0.4

    shift_scale_rotate:
      enabled: true
      shift_limit: 0.12
      scale_limit: 0.25
      rotate_limit: 20
      p: 0.5

    brightness_contrast:
      enabled: true
      brightness_limit: 0.25
      contrast_limit: 0.25
      p: 0.5

    hue_saturation_value:
      enabled: true
      hue_shift_limit: 15
      sat_shift_limit: 20
      val_shift_limit: 15
      p: 0.3

    gaussian_blur:
      enabled: true
      blur_limit: [3, 7]
      p: 0.2

    random_gamma:
      enabled: true
      gamma_limit: [80, 120]
      p: 0.3

    gaussian_noise:
      enabled: true
      var_limit: [10.0, 30.0]
      p: 0.2

# ğŸ“Š MEJORAS ESPERADAS vs conservador:
# - IoU esperado: 88-90% (mismo objetivo)
# - Convergencia: 15-20% mÃ¡s rÃ¡pida
#   * Epochs necesarios: ~70-80 (vs 80-90 conservador)
#   * Early stopping puede activarse epoch 60-70
# - Tiempo total: 12-15 horas (vs 15-18h conservador)
#
# TIEMPO ESTIMADO DETALLADO:
# - Convergencia esperada: epoch 60-75 (vs 70-85)
# - Con RTX 3090/4090: ~8-10 min/epoch â†’ ~12-15 horas
# - Con A100: ~5-7 min/epoch â†’ ~8-11 horas
# - Con V100 32GB: ~10-12 min/epoch â†’ ~15-18 horas
#
# REQUISITOS:
# - VRAM: 18-20GB usado (mismo que conservador)
# - RAM: 24GB+ recomendado
# - Disco: ~15GB
# - GPU: RTX 3090, RTX 4090, A5000, V100 32GB, A100, H100
# - âš ï¸ **Experiencia**: Recomendado tener experiencia monitoreando entrenamientos
#
# ğŸ¯ MONITOREO CRÃTICO (MÃS IMPORTANTE QUE CONSERVADOR):
#
# âš ï¸ **EPOCHS 1-5 (WARMUP)**:
# - Loss debe BAJAR gradualmente (no saltos errÃ¡ticos)
# - Si loss diverge (NaN, Inf, >10): PARAR y reducir LR inicial
# - IoU epoch 5 deberÃ­a estar >0.60
#
# âš ï¸ **EPOCHS 10-15 (POST-WARMUP)**:
# - IoU deberÃ­a estar >0.80 (vs 0.78 conservador)
# - Si <0.75: LR muy alto, reducir a 0.00025
# - Loss debe bajar suavemente, no oscilar
#
# âš ï¸ **EPOCHS 30-40 (MID-TRAINING)**:
# - IoU deberÃ­a estar >0.85
# - Validar calidad visual de bordes (no debe degradar)
# - Si bordes borrosos: LR alto afectÃ³ calidad
#
# âš ï¸ **EPOCHS 60-75 (CONVERGENCIA)**:
# - IoU deberÃ­a alcanzar >0.87
# - Early stopping puede activarse
# - Si no converge: patience muy bajo, extender a 18
#
# ğŸš¨ SEÃ‘ALES DE PROBLEMA:
# 1. **Loss diverge en warmup**: Reducir warmup_start_lr a 0.00003
# 2. **Oscilaciones grandes**: Reducir LR a 0.00025
# 3. **Bordes borrosos**: LR muy agresivo, volver a conservador
# 4. **No mejora despuÃ©s epoch 40**: Aumentar patience a 18
# 5. **Validation loss aumenta**: Overfitting, aumentar weight_decay
#
# ğŸ¯ CUÃNDO USAR ESTA CONFIG:
# âœ… Ya probaste config conservador y funcionÃ³
# âœ… Tienes experiencia monitoreando entrenamientos
# âœ… Quieres resultados mÃ¡s rÃ¡pido (12-15h vs 15-18h)
# âœ… Puedes intervenir si hay problemas
# âœ… Hardware estable (no va a interrumpirse)
#
# âŒ NO USAR SI:
# âŒ Primera vez con este dataset
# âŒ No puedes monitorear activamente
# âŒ Hardware inestable (puede interrumpirse)
# âŒ Prefieres seguridad sobre velocidad
#
# ğŸ”„ SI ESTA CONFIG NO FUNCIONA:
# 1. Reducir LR a 0.00025 (intermedio)
# 2. Aumentar warmup a 7 epochs
# 3. Volver a config conservador
#
# ğŸ“š REFERENCIAS:
# - "Accurate, Large Minibatch SGD" (Goyal et al., 2017)
#   â†’ Linear scaling funciona con warmup adecuado
# - "Bag of Tricks for Image Classification" (He et al., 2018)
#   â†’ Warmup crÃ­tico para LR altos en vision tasks
# - "Fast.ai lessons" (Howard & Gugger, 2020)
#   â†’ LR finder + aggressive training strategies
#
# ğŸ’¡ TIP AVANZADO:
# Si tienes tiempo, hacer "LR range test" primero:
# 1. Entrenar 1 epoch con LR creciente (0.0001 â†’ 0.001)
# 2. Graficar loss vs LR
# 3. Elegir LR justo antes de divergencia
# 4. Esto puede revelar LR Ã³ptimo especÃ­fico para tu GPU/setup

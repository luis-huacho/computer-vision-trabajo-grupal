# ConfiguraciÃ³n OPTIMIZADA para 100% AISegment Dataset - 20GB VRAM (CONSERVADOR)
# Dataset: 34,425 imÃ¡genes completas
# Hardware: GPU con 20GB VRAM (RTX 3090, RTX 4090, A5000, V100 32GB, A100)
# Uso: python main.py train --config aisegment_full_optimized_20gb
#
# ESTRATEGIA: CONSERVADOR (âˆšbatch scaling)
# - Batch size 24 (duplicado de 12 para aprovechar VRAM extra)
# - Learning rate 0.00021 (escalado con âˆš2 para estabilidad)
# - Sin warmup (âˆšbatch scaling es naturalmente estable)
# - Convergencia suave y predecible
#
# VENTAJAS vs 10GB config:
# - 40-50% mÃ¡s rÃ¡pido (batch 24 vs 12)
# - Gradientes mÃ¡s estables (mejor Batch Norm)
# - +0.5-1% IoU esperado
# - Tiempo: ~15-18 horas (vs 25-30h)
#
# Objetivo: IoU 88-90% con convergencia estable

experiment:
  name: "AISegment 100% - 20GB VRAM Optimized (Conservador)"
  description: "Entrenamiento con dataset completo - Batch 24, LR escalado âˆšbatch"
  mode: "production"

model:
  architecture: "resnet50"
  image_size: 384
  use_pretrained: true
  use_attention: true

dataset:
  type: "aisegment"
  root: "datasets/AISegment"

  # Descarga automÃ¡tica con kagglehub
  auto_download: true
  kaggle_dataset_id: "laurentmih/aisegmentcom-matting-human-datasets"

  # Split train/val (80/20)
  train_val_split: 0.8

  # Dataset completo (sin sampling)
  sampling:
    enabled: false
    mode: "full"

training:
  num_epochs: 100      # Mismo que config 10GB (batch mayor compensa)
  batch_size: 24       # ğŸ”¥ DUPLICADO de 12 (aprovecha 20GB VRAM)
  learning_rate: 0.00021  # â¬†ï¸ Escalado âˆšbatch: 0.00015 Ã— âˆš2 â‰ˆ 0.00021
  weight_decay: 0.0000005  # Mantener (regularizaciÃ³n adecuada)
  num_workers: 10      # â¬†ï¸ Aumentado de 8 (mejor I/O con batch mayor)
  pin_memory: true
  drop_last: true
  gradient_clip_max_norm: 0.5
  mixed_precision: true  # âœ… Mantener (ahorra VRAM adicional)

  scheduler:
    type: "cosine_annealing"
    T_0: 12            # â¬‡ï¸ Reducido de 15 (batch mayor = convergencia mÃ¡s rÃ¡pida)
    T_mult: 2
    eta_min: 0.000005  # LR mÃ­nimo para fine-tuning

  # Loss weights optimizados (sin cambios - ya son Ã³ptimos)
  loss_weights:
    alpha: 0.7         # BCE
    beta: 1.3          # Dice (overlap crÃ­tico)
    gamma: 0.7         # Perceptual (calidad visual)
    delta: 0.5         # Edge (bordes precisos)

validation:
  frequency: 1
  early_stopping:
    enabled: true
    patience: 20       # Mantener (conservador)
    min_delta: 0.00005
  target_metrics:
    iou: 0.88          # Target SOTA
    dice: 0.92
    pixel_accuracy: 0.96

checkpoints:
  save_best: true
  save_last: true
  save_every_n_epochs: 10
  checkpoint_dir: "checkpoints/aisegment_full_optimized_20gb"

logging:
  log_every_n_batches: 20  # â¬‡ï¸ Reducido de 25 (monitoreo mÃ¡s frecuente)
  save_plots: true

# Augmentaciones agresivas (sin cambios - ya optimizadas)
augmentation:
  train:
    horizontal_flip: 0.5
    random_rotate90: 0.4

    shift_scale_rotate:
      enabled: true
      shift_limit: 0.12
      scale_limit: 0.25
      rotate_limit: 20
      p: 0.5

    brightness_contrast:
      enabled: true
      brightness_limit: 0.25
      contrast_limit: 0.25
      p: 0.5

    hue_saturation_value:
      enabled: true
      hue_shift_limit: 15
      sat_shift_limit: 20
      val_shift_limit: 15
      p: 0.3

    gaussian_blur:
      enabled: true
      blur_limit: [3, 7]
      p: 0.2

    random_gamma:
      enabled: true
      gamma_limit: [80, 120]
      p: 0.3

    gaussian_noise:
      enabled: true
      var_limit: [10.0, 30.0]
      p: 0.2

# ğŸ“Š MEJORAS ESPERADAS vs 10GB config:
# - IoU esperado: 88-90% (mismo objetivo, convergencia mÃ¡s estable)
# - Velocidad: 40-50% mÃ¡s rÃ¡pido
#   * Batch 24 vs 12 = ~2x throughput teÃ³rico
#   * Overhead reduce ganancia a ~1.5x real
# - Calidad gradientes: +15-20% estabilidad (Batch Norm con 24 samples)
# - Tiempo total: 15-18 horas (vs 25-30h con batch 12)
#
# TIEMPO ESTIMADO DETALLADO:
# - ~27,540 imÃ¡genes de train (80% de 34,425)
# - Batch size 24 â†’ ~1,148 batches/epoch (vs 2,295 con batch 12)
# - 100 epochs â†’ ~114,800 batches totales
# - Con RTX 3090/4090: ~10-12 min/epoch â†’ ~16-20 horas
# - Con A100: ~6-8 min/epoch â†’ ~10-13 horas
# - Con V100 32GB: ~12-15 min/epoch â†’ ~20-25 horas
#
# REQUISITOS:
# - VRAM: 18-20GB usado (batch 24 + mixed precision + modelo)
# - RAM: 24GB+ recomendado (num_workers 10 + dataset caching)
# - Disco: ~15GB (dataset + checkpoints + logs)
# - GPU: RTX 3090, RTX 4090, A5000, V100 32GB, A100, H100
#
# MONITOREO RECOMENDADO:
# - Epoch 1-5: Validar que no hay OOM
#   * Si OOM: reducir batch_size a 20 o num_workers a 8
# - Epoch 10-15: IoU deberÃ­a estar >0.82
#   * Si <0.78: revisar datos o aumentar LR ligeramente
# - Epoch 30-40: IoU deberÃ­a estar >0.85
# - Convergencia esperada: epoch 60-80
# - Early stopping: safety net (patience 20)
#
# ğŸ¯ CUÃNDO USAR ESTA CONFIG:
# âœ… Primera vez entrenando con 20GB VRAM
# âœ… Quieres convergencia predecible y estable
# âœ… No tienes prisa (100 epochs completos)
# âœ… Prefieres seguridad sobre velocidad
#
# ğŸ”„ SI ESTA CONFIG FUNCIONA BIEN:
# Considera probar: aisegment_full_optimized_20gb_aggressive.yaml
# - LR mÃ¡s alto (0.0003)
# - Converge ~15-20% mÃ¡s rÃ¡pido
# - Requiere mÃ¡s monitoreo
#
# ğŸ“š REFERENCIAS:
# - "Accurate, Large Minibatch SGD" (Goyal et al., 2017)
#   â†’ Recomiendan âˆšbatch scaling para tareas visuales
# - "Don't Decay LR, Increase Batch Size" (Smith et al., 2018)
#   â†’ Batch grande + LR escalado = mejor generalizaciÃ³n

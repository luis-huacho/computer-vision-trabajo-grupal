# Conceptos T√©cnicos y M√©tricas

Este documento explica los conceptos fundamentales, arquitecturas y m√©tricas utilizadas en el proyecto de remoci√≥n de fondo con U-Net Autoencoder.

## üìö Conceptos Fundamentales

### U-Net Architecture

**Definici√≥n**: U-Net es una arquitectura de red neuronal convolucional dise√±ada originalmente para segmentaci√≥n biom√©dica, caracterizada por su forma de "U" que combina un path de contracci√≥n (encoder) con un path de expansi√≥n (decoder).

**Componentes Clave**:
- **Contracting Path (Encoder)**: Captura el contexto mediante convoluciones y pooling
- **Expansive Path (Decoder)**: Permite localizaci√≥n precisa mediante upsampling
- **Skip Connections**: Conectan directamente capas del encoder con el decoder

**Ventajas**:
- Preserva detalles finos gracias a las skip connections
- Eficiente con pocos datos de entrenamiento
- Excelente para tareas de segmentaci√≥n pixel-wise

### Autoencoder

**Definici√≥n**: Arquitectura neuronal que aprende representaciones comprimidas de los datos (encoding) y luego los reconstruye (decoding).

**Estructura**:
```
Input ‚Üí Encoder ‚Üí Latent Space ‚Üí Decoder ‚Üí Output
```

**En nuestro contexto**:
- **Encoder**: Extrae caracter√≠sticas de la imagen original
- **Latent Space**: Representaci√≥n comprimida con informaci√≥n esencial
- **Decoder**: Reconstruye la imagen enfoc√°ndose solo en personas

### Attention Mechanisms

**Attention Gates**: Mecanismo que permite al modelo "prestar atenci√≥n" a regiones espec√≠ficas de la imagen.

**Funcionamiento**:
1. Recibe feature maps del encoder y decoder
2. Calcula coeficientes de atenci√≥n
3. Multiplica los feature maps por estos coeficientes
4. Enfatiza regiones relevantes (personas) y suprime fondo

**F√≥rmula matem√°tica**:
```
Œ± = œÉ(W_g * g + W_x * x + b)
output = x * Œ±
```

Donde:
- `g`: gating signal (del decoder)
- `x`: feature map (del encoder)
- `œÉ`: funci√≥n sigmoid
- `Œ±`: coeficientes de atenci√≥n

### Transfer Learning

**Concepto**: Utilizar un modelo pre-entrenado en una tarea relacionada como punto de partida.

**En nuestro modelo**:
- **Backbone**: ResNet50 pre-entrenado en ImageNet
- **Ventajas**: Convergencia m√°s r√°pida, mejor extracci√≥n de caracter√≠sticas
- **Adaptaci√≥n**: Las √∫ltimas capas se ajustan para nuestra tarea espec√≠fica

## üéØ Funciones de P√©rdida

### 1. Binary Cross-Entropy (BCE) Loss

**Prop√≥sito**: Clasificaci√≥n binaria pixel-wise (persona vs fondo)

**F√≥rmula**:
```
BCE = -[y*log(p) + (1-y)*log(1-p)]
```

**Interpretaci√≥n**:
- `y`: etiqueta real (0 o 1)
- `p`: probabilidad predicha
- **Valor √≥ptimo**: 0.0
- **Rango t√≠pico**: 0.0 - 2.0

### 2. Dice Loss

**Prop√≥sito**: Maneja desbalance de clases en segmentaci√≥n

**F√≥rmula**:
```
Dice = (2 * |A ‚à© B|) / (|A| + |B|)
Dice Loss = 1 - Dice
```

**Interpretaci√≥n**:
- `A`: predicci√≥n
- `B`: ground truth
- **Valor √≥ptimo**: 0.0 (Dice Coefficient = 1.0)
- **Rango**: 0.0 - 1.0

**Ventajas**:
- Robusto ante desbalance de clases
- Se enfoca en la superposici√≥n de regiones

### 3. Perceptual Loss

**Prop√≥sito**: Preservar caracter√≠sticas visuales naturales

**Funcionamiento**:
- Utiliza features de una red pre-entrenada (VGG)
- Compara representaciones de alto nivel
- Mantiene estructura y textura visual

**Aplicaci√≥n**:
```python
perceptual_loss = MSE(VGG_features(pred), VGG_features(target))
```

### 4. Edge Loss

**Prop√≥sito**: Preservar contornos n√≠tidos en la segmentaci√≥n

**Implementaci√≥n**:
- Aplica filtros Sobel para detectar bordes
- Compara gradientes entre predicci√≥n y ground truth
- Penaliza bordes difusos o incorrectos

**Filtros Sobel**:
```
Sobel_x = [[-1, 0, 1],    Sobel_y = [[-1, -2, -1],
           [-2, 0, 2],               [ 0,  0,  0],
           [-1, 0, 1]]               [ 1,  2,  1]]
```

### Funci√≥n de P√©rdida Compuesta

**Combinaci√≥n**:
```
Total_Loss = Œ±*BCE + Œ≤*Dice + Œ≥*Perceptual + Œ¥*Edge
```

**Pesos t√≠picos**:
- Œ± = 1.0 (BCE)
- Œ≤ = 1.0 (Dice)
- Œ≥ = 0.5 (Perceptual)
- Œ¥ = 0.3 (Edge)

## üìä M√©tricas de Evaluaci√≥n

### 1. Intersection over Union (IoU)

**Definici√≥n**: Mide la superposici√≥n entre predicci√≥n y ground truth

**F√≥rmula**:
```
IoU = |A ‚à© B| / |A ‚à™ B|
```

**Interpretaci√≥n**:
- **Rango**: 0.0 - 1.0
- **0.0**: Sin superposici√≥n
- **0.5**: Aceptable
- **0.7**: Bueno
- **0.85+**: Excelente

**Umbral de decisi√≥n**: T√≠picamente 0.5 para binarizaci√≥n

### 2. Dice Coefficient

**Definici√≥n**: Medida de similitud entre dos conjuntos

**F√≥rmula**:
```
Dice = (2 * |A ‚à© B|) / (|A| + |B|)
```

**Interpretaci√≥n**:
- **Rango**: 0.0 - 1.0
- **0.0**: Sin similitud
- **0.8**: Buena segmentaci√≥n
- **0.9+**: Excelente segmentaci√≥n

**Relaci√≥n con IoU**:
```
Dice = 2*IoU / (1 + IoU)
```

### 3. Pixel Accuracy

**Definici√≥n**: Proporci√≥n de p√≠xeles clasificados correctamente

**F√≥rmula**:
```
Pixel_Accuracy = (TP + TN) / (TP + TN + FP + FN)
```

**Interpretaci√≥n**:
- **Rango**: 0.0 - 1.0
- **0.95+**: Muy buena precisi√≥n
- **Limitaci√≥n**: Puede ser enga√±osa con clases desbalanceadas

### 4. M√©tricas Adicionales

#### Sensitivity (Recall)
```
Sensitivity = TP / (TP + FN)
```
Mide la capacidad de detectar p√≠xeles de persona.

#### Specificity
```
Specificity = TN / (TN + FP)
```
Mide la capacidad de identificar correctamente el fondo.

#### Precision
```
Precision = TP / (TP + FP)
```
Proporci√≥n de p√≠xeles predichos como persona que realmente lo son.

#### F1-Score
```
F1 = 2 * (Precision * Recall) / (Precision + Recall)
```
Media arm√≥nica entre precision y recall.

## üîß Par√°metros de Entrenamiento

### Learning Rate Scheduling

**Cosine Annealing with Warm Restarts**:
```python
scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)
```

**Comportamiento**:
- Reduce learning rate siguiendo una funci√≥n coseno
- "Reinicia" peri√≥dicamente para escapar m√≠nimos locales
- T_0: per√≠odo inicial, T_mult: multiplicador de per√≠odo

### Optimizaci√≥n

**Adam Optimizer**:
- **Learning Rate**: 1e-4 (t√≠pico para fine-tuning)
- **Weight Decay**: 1e-5 (regularizaci√≥n L2)
- **Betas**: (0.9, 0.999) (momentos por defecto)

### Regularizaci√≥n

**Dropout**: 
- Encoder: 0.1
- Bottleneck: 0.2
- Previene overfitting

**Batch Normalization**:
- Estabiliza entrenamiento
- Permite learning rates m√°s altos
- Act√∫a como regularizador

## üìà Interpretaci√≥n de Resultados

### Curvas de Entrenamiento

#### Loss Curves
- **Descenso suave**: Entrenamiento estable
- **Oscilaciones**: Learning rate muy alto
- **Plateau**: Posible convergencia o learning rate muy bajo
- **Divergencia**: Gradientes inestables

#### Validation vs Training
- **Gap peque√±o**: Buen balance, sin overfitting
- **Gap grande**: Overfitting, necesita m√°s regularizaci√≥n
- **Val > Train**: Posible problema en datos o implementaci√≥n

### M√©tricas por √âpoca

#### IoU Progression
- **Inicio**: ~0.3 - 0.4
- **Convergencia**: 0.85+
- **Estancamiento en 0.7**: Posible problema de arquitectura

#### Dice Coefficient
- **Correlacionado con IoU**: Debe seguir tendencia similar
- **M√°s sensible**: Cambios m√°s pronunciados que IoU
- **Objetivo**: 0.9+ para aplicaciones comerciales

## üé® Aumentaci√≥n de Datos

### Transformaciones Geom√©tricas
- **HorizontalFlip**: Duplica variabilidad, preserva personas
- **RandomRotate90**: Diferentes orientaciones
- **ShiftScaleRotate**: Variaciones realistas de pose

### Transformaciones Fotom√©tricas
- **RandomBrightnessContrast**: Simula diferentes condiciones de luz
- **HueSaturationValue**: Variabilidad de color
- **GaussianBlur**: Robustez ante desenfoques

### Consideraciones Especiales
- **Preservar anotaciones**: Transformaciones deben aplicarse a imagen y m√°scara
- **Probabilidades balanceadas**: No todas las transformaciones en cada muestra
- **Coherencia temporal**: En videos, mantener consistencia

## ‚ö° Optimizaciones de Rendimiento

### Mixed Precision Training

**Concepto**: Usar FP16 en lugar de FP32 para acelerar entrenamiento

**Implementaci√≥n**:
```python
from torch.cuda.amp import GradScaler, autocast

scaler = GradScaler()
with autocast():
    outputs = model(inputs)
    loss = criterion(outputs, targets)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

**Beneficios**:
- 1.5-2x m√°s r√°pido
- Usa menos memoria GPU
- Mantiene precisi√≥n num√©rica

### Gradient Checkpointing

**Prop√≥sito**: Reducir uso de memoria a costa de tiempo de c√≥mputo

**Aplicaci√≥n**:
```python
import torch.utils.checkpoint as checkpoint

def forward(self, x):
    x = checkpoint.checkpoint(self.encoder, x)
    return self.decoder(x)
```

**Trade-off**:
- ‚Üì Memoria: ~50% menos
- ‚Üë Tiempo: ~20% m√°s lento

### Data Loading Optimization

**Estrategias**:
- **num_workers**: 4-8 procesos paralelos
- **pin_memory**: True para GPU
- **prefetch_factor**: 2-4 para cache

**Implementaci√≥n**:
```python
DataLoader(
    dataset,
    batch_size=8,
    num_workers=4,
    pin_memory=True,
    prefetch_factor=2
)
```

## üß™ Experimentaci√≥n y Debugging

### Sanity Checks

#### 1. Overfitting Test
```python
# Entrenar en 1 batch peque√±o
small_dataset = Subset(dataset, range(8))
# Debe alcanzar loss ~0 r√°pidamente
```

#### 2. Learning Rate Range Test
```python
# Incrementar LR exponencialmente
for epoch in range(100):
    lr = 1e-6 * (10 ** (epoch / 100))
    # Plotear loss vs LR
```

#### 3. Gradient Flow
```python
# Verificar gradientes no son 0 o infinito
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"{name}: {param.grad.norm()}")
```

### Common Issues y Soluciones

#### Loss no disminuye
- **Verificar**: Learning rate, arquitectura, datos
- **Soluci√≥n**: Reducir LR, simplificar modelo, verificar labels

#### Overfitting r√°pido
- **S√≠ntomas**: Val loss aumenta mientras train loss baja
- **Soluci√≥n**: M√°s dropout, regularizaci√≥n, m√°s datos

#### Gradientes explosivos
- **S√≠ntomas**: Loss se vuelve NaN
- **Soluci√≥n**: Gradient clipping, LR m√°s bajo

#### Convergencia lenta
- **Causas**: LR muy bajo, modelo muy profundo
- **Soluci√≥n**: LR scheduling, skip connections

## üìã Evaluaci√≥n Avanzada

### Cross-Validation

**K-Fold Validation**:
```python
from sklearn.model_selection import KFold

kfold = KFold(n_splits=5, shuffle=True, random_state=42)
for train_idx, val_idx in kfold.split(dataset):
    # Entrenar y evaluar cada fold
```

**Estratified Split**: Para datasets desbalanceados
```python
from sklearn.model_selection import StratifiedKFold
# Mantiene proporci√≥n de clases en cada fold
```

### Benchmarking

#### Comparaci√≥n con Baselines
- **Traditional Methods**: GrabCut, Watershed
- **Deep Learning**: DeepLab, Mask R-CNN
- **Specialized**: Background Matting, MODNet

#### M√©tricas de Tiempo
```python
import time

start_time = time.time()
output = model(input_batch)
inference_time = time.time() - start_time

fps = batch_size / inference_time
```

### Test-Time Augmentation (TTA)

**Concepto**: Promediar predicciones de m√∫ltiples transformaciones

**Implementaci√≥n**:
```python
def tta_predict(model, image, transforms):
    predictions = []
    for transform in transforms:
        aug_image = transform(image)
        pred = model(aug_image)
        # Invertir transformaci√≥n
        pred = inverse_transform(pred, transform)
        predictions.append(pred)
    
    return torch.mean(torch.stack(predictions), dim=0)
```

## üéØ M√©tricas Espec√≠ficas para Remoci√≥n de Fondo

### Trimap Accuracy

**Definici√≥n**: Precisi√≥n en regiones de transici√≥n (bordes)

**C√°lculo**:
1. Generar trimap (foreground, background, unknown)
2. Evaluar precisi√≥n solo en regi√≥n "unknown"
3. M√°s desafiante que m√©tricas est√°ndar

### Alpha Matte Quality

**Sum of Absolute Differences (SAD)**:
```
SAD = Œ£|Œ±_pred - Œ±_true|
```

**Mean Squared Error (MSE)**:
```
MSE = (1/N) * Œ£(Œ±_pred - Œ±_true)¬≤
```

**Gradient Error**:
```
Grad_Error = Œ£|‚àáŒ±_pred - ‚àáŒ±_true|
```

### Perceptual Metrics

#### Structural Similarity (SSIM)
```python
from skimage.metrics import structural_similarity

ssim_value = structural_similarity(pred_rgb, target_rgb, multichannel=True)
```

**Interpretaci√≥n**:
- **Rango**: -1 a 1
- **0.9+**: Excelente similitud visual
- **0.7-0.9**: Buena calidad
- **<0.7**: Calidad pobre

#### Learned Perceptual Image Patch Similarity (LPIPS)
```python
import lpips

loss_fn = lpips.LPIPS(net='alex')
perceptual_distance = loss_fn(pred_tensor, target_tensor)
```

**Ventajas**:
- Correlaciona mejor con percepci√≥n humana
- Detecta diferencias sem√°nticamente importantes

## üîç An√°lisis de Casos Edge

### Escenarios Desafiantes

#### 1. M√∫ltiples Personas
- **Problema**: Separaci√≥n entre personas
- **Soluci√≥n**: Instance segmentation, mejores skip connections

#### 2. Oclusi√≥n Parcial
- **Problema**: Personas parcialmente ocultas
- **Soluci√≥n**: M√°s datos de entrenamiento, context understanding

#### 3. Backgrounds Complejos
- **Problema**: Confusi√≥n con texturas similares a piel/ropa
- **Soluci√≥n**: M√°s diversidad en datos, mejores features

#### 4. Iluminaci√≥n Extrema
- **Problema**: Sombras duras, contraluz
- **Soluci√≥n**: Augmentaci√≥n fotom√©trica, normalizaci√≥n adaptiva

### Estrategias de Mejora

#### 1. Active Learning
- Identificar casos problem√°ticos autom√°ticamente
- Priorizar anotaci√≥n manual de casos dif√≠ciles

#### 2. Hard Negative Mining
- Enfocarse en ejemplos donde el modelo falla
- Rebalancear dataset hacia casos desafiantes

#### 3. Multi-Scale Training
- Entrenar con diferentes resoluciones
- Mejor generalizaci√≥n a diferentes tama√±os

## üìä Monitoreo en Producci√≥n

### M√©tricas de Sistema

#### Latencia
- **Target**: <100ms para aplicaciones en tiempo real
- **Medici√≥n**: Percentil 95, no solo promedio

#### Throughput
- **M√©trica**: Im√°genes por segundo (IPS)
- **Optimizaci√≥n**: Batch processing, model optimization

#### Uso de Recursos
- **GPU Memory**: Monitorear picos y leaks
- **CPU Usage**: Para pre/post-procesamiento
- **Disk I/O**: Para carga de modelos grandes

### Drift Detection

#### Data Drift
- Distribuci√≥n de inputs cambia con el tiempo
- **Detecci√≥n**: Statistical tests, embedding analysis

#### Model Drift
- Performance se degrada gradualmente
- **Detecci√≥n**: A/B testing, performance monitoring

### Calidad Autom√°tica

#### No-Reference Metrics
- M√©tricas sin ground truth
- **Ejemplos**: Blur detection, edge consistency

#### Anomaly Detection
- Detecci√≥n de outputs an√≥malos
- **M√©todos**: Reconstruction error, density estimation

## üöÄ Optimizaci√≥n para Deployment

### Model Quantization

**Post-Training Quantization**:
```python
import torch.quantization

model.eval()
model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
torch.quantization.prepare(model, inplace=True)
torch.quantization.convert(model, inplace=True)
```

**Beneficios**:
- 4x menos memoria
- 2-4x m√°s r√°pido en CPU
- Ligera p√©rdida de precisi√≥n

### Model Pruning

**Structured Pruning**:
```python
import torch.nn.utils.prune as prune

prune.ln_structured(model.conv1, name="weight", amount=0.3, n=2, dim=0)
```

**Unstructured Pruning**:
```python
prune.random_unstructured(model.conv1, name="weight", amount=0.3)
```

### Knowledge Distillation

**Concepto**: Entrenar modelo peque√±o (student) con modelo grande (teacher)

```python
def distillation_loss(student_logits, teacher_logits, temperature=3):
    soft_targets = F.softmax(teacher_logits / temperature, dim=1)
    soft_student = F.log_softmax(student_logits / temperature, dim=1)
    return F.kl_div(soft_student, soft_targets, reduction='batchmean')
```

## üìö Recursos Adicionales

### Datasets Complementarios
- **COCO Person**: M√°s diversidad
- **ADE20K**: Escenas complejas
- **Open Images**: Escala masiva
- **PASCAL VOC**: Benchmark est√°ndar

### Herramientas de Visualizaci√≥n
- **TensorBoard**: M√©tricas y gr√°ficas
- **Weights & Biases**: Experiment tracking
- **Visdom**: Visualizaci√≥n en tiempo real

### Frameworks de Optimizaci√≥n
- **TensorRT**: NVIDIA GPU optimization
- **ONNX**: Cross-platform deployment
- **OpenVINO**: Intel hardware optimization

---

**Nota**: Este documento es din√°mico y debe actualizarse conforme evolucione el proyecto y se descubran nuevas t√©cnicas o mejores pr√°cticas.